{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0d3686-016d-4dad-b7ce-9df870d7cfd2",
   "metadata": {},
   "source": [
    "# Introduction to BeautifulSoup and Scrapy\n",
    "\n",
    "## BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a Python library used for web scraping and parsing HTML and XML documents. It provides simple methods for navigating, searching, and modifying the parsed data. It is best suited for small-scale web scraping projects where the website structure is not too complex. Since it does not handle HTTP requests by itself, it is often used alongside the `requests` library.\n",
    "\n",
    "### Key Features:\n",
    "- Easy to set up and use\n",
    "- Parses HTML and XML efficiently\n",
    "- Best for small to medium-scale scraping tasks\n",
    "- Requires external libraries for handling requests\n",
    "\n",
    "## MechanicalSoup\n",
    "\n",
    "MechanicalSoup is a Python library used for automating interactions with websites and scraping content. It combines the power of **Requests** and **BeautifulSoup** to allow for easy navigation, form submission, and content extraction from web pages.\n",
    "\n",
    "### Key Features:\n",
    "- **Simple API**: MechanicalSoup provides a straightforward and user-friendly interface to interact with websites, making it easy for both beginners and advanced users.\n",
    "- **Form Handling**: Unlike Scrapy, MechanicalSoup makes it easy to fill out and submit forms on websites, enabling the automation of login and data entry tasks.\n",
    "- **HTML Parsing**: It uses **BeautifulSoup** to parse HTML, providing the flexibility to extract data from websites with complex structures.\n",
    "- **Stateful Sessions**: Supports stateful sessions, meaning that it can manage cookies and session data, keeping track of user logins and maintaining the context between requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5b4c5d-fa59-4b28-9c3d-62ff88bb67c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed: 64 articles saved in 6.00 seconds using BeautifulSoup.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Define BBC News URL and request headers\n",
    "BBC_NEWS_URL = 'https://www.bbc.com/news'\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fetch and parse the main BBC News page\n",
    "response = requests.get(BBC_NEWS_URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract article links\n",
    "article_links = set()\n",
    "for anchor in soup.find_all('a', href=True):\n",
    "    href = anchor['href']\n",
    "    if href.startswith('/news/'):\n",
    "        article_links.add(f'https://www.bbc.com{href}')\n",
    "\n",
    "# Scrape details from each article\n",
    "articles_data = []\n",
    "for article_url in list(article_links)[:500]:\n",
    "    try:\n",
    "        article_response = requests.get(article_url, headers=HEADERS)\n",
    "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "\n",
    "        title = article_soup.find('h1').get_text(strip=True) if article_soup.find('h1') else 'Title Not Found'\n",
    "        content = ' '.join([p.get_text(strip=True) for p in article_soup.find_all('p')])\n",
    "\n",
    "        articles_data.append({'title': title, 'url': article_url, 'content': content})\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Save extracted data to a CSV file\n",
    "csv_filename = 'bbc_news_scraped_data.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=['title', 'url', 'content'])\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(articles_data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(articles_data)} articles saved in {end_time - start_time:.2f} seconds using BeautifulSoup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c0367e-86f0-4ed8-8df9-e6d472ca1f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed using MechanicalSoup: 64 articles saved in 2.66 seconds.\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize a browser object\n",
    "browser = mechanicalsoup.Browser()\n",
    "BBC_NEWS_URL = 'https://www.bbc.com/news'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Fetch the main BBC News page\n",
    "page = browser.get(BBC_NEWS_URL)\n",
    "soup = page.soup\n",
    "\n",
    "# Extract article links\n",
    "article_links = list(set(f'https://www.bbc.com{a[\"href\"]}' for a in soup.select('a[href^=\"/news/\"]')))\n",
    "\n",
    "# Scrape article details\n",
    "data = []\n",
    "for article_url in article_links[:500]:\n",
    "    try:\n",
    "        article_page = browser.get(article_url)\n",
    "        article_soup = article_page.soup\n",
    "\n",
    "        title = article_soup.find('h1').get_text(strip=True) if article_soup.find('h1') else 'Title Not Found'\n",
    "        content = ' '.join(p.get_text(strip=True) for p in article_soup.find_all('p'))\n",
    "\n",
    "        data.append({'title': title, 'url': article_url, 'content': content})\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Save extracted data to a CSV file\n",
    "csv_filename = 'bbc_news_articles_mechanicalsoup.csv'\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=['title', 'url', 'content'])\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed using MechanicalSoup: {len(data)} articles saved in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63537df-eccc-4328-83f4-41a7d39307bf",
   "metadata": {},
   "source": [
    "# Performance Comparison: BeautifulSoup vs. MechanicalSoup\n",
    "\n",
    "| Feature               | BeautifulSoup                                     | MechanicalSoup                                        |\n",
    "|-----------------------|--------------------------------------------------|------------------------------------------------------|\n",
    "| **Total Articles Scraped** | 64                                             | 64                                                   |\n",
    "| **Execution Time (s)**  | 6.0                                             | 2.66                                                 |\n",
    "| **Setup Complexity**   | Simple setup with minimal dependencies          | Simple setup, requires minimal configuration         |\n",
    "| **Processing Speed**   | Slower for handling large datasets              | Faster, optimized for web scraping with stateful sessions |\n",
    "| **Scalability**        | Best for small to medium-sized tasks            | Suited for small to medium-scale tasks, not as scalable as Scrapy |\n",
    "| **Code Complexity**    | Straightforward code but requires manual request handling | Slightly more complex but automates browsing and form submissions |\n",
    "| **Built-in Capabilities** | Lacks native crawling and session management, needs additional setup | Includes built-in crawling, form handling, and stateful sessions |\n",
    "| **Best Use Case**      | Suitable for small-scale projects or quick one-off extractions | Great for simple interactive web scraping, form handling, and session management |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e8dd4-258e-42e7-8ccd-f7b178b65f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
